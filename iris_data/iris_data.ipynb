{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# February, 2018\n",
    "# https://github.com/itsergiu/Tensorflow/tree/master/iris_data\n",
    "# https://www.tensorflow.org/get_started/get_started_for_beginners\n",
    "# https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause. The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this appplication is to provide Tensorflow iris_data.py code in format of Jupyter Notebook.\n",
    "\n",
    "This may help you to speed up your tests and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "Tensorflow version  1.5.0\n"
     ]
    }
   ],
   "source": [
    "# Check library version\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "print(\"Tensorflow version \", tf.VERSION) # Program requires at least TensorFlow v1.4\n",
    "# Upgrading or installing Tensorflow with Anaconda:\n",
    "# https://www.tensorflow.org/install/install_windows#installing_with_anaconda\n",
    "# Alternatively you may run in Anaconda promt: pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def maybe_download():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# The remainder of this file contains a simple example of a csv parser,\n",
    "#     implemented using a the `Dataset` class.\n",
    "\n",
    "# `tf.parse_csv` sets the types of the outputs to match the examples given in\n",
    "#     the `record_defaults` argument.\n",
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('Species')\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size):\n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "\n",
    "    # Parse each line.\n",
    "    dataset = dataset.map(_parse_line)\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call load_data() to parse the CSV file.\n",
    "(train_feature, train_label), (test_feature, test_label) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code to create a feature column:\n",
    "my_feature_columns = [\n",
    "    tf.feature_column.numeric_column(key='SepalLength'),\n",
    "    tf.feature_column.numeric_column(key='SepalWidth'),\n",
    "    tf.feature_column.numeric_column(key='PetalLength'),\n",
    "    tf.feature_column.numeric_column(key='PetalWidth')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\SIATCO~1.SLA\\AppData\\Local\\Temp\\tmp852m4dl6\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\SIATCO~1.SLA\\\\AppData\\\\Local\\\\Temp\\\\tmp852m4dl6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002192C4F6C50>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# This Estimator builds a neural network that classifies examples. \n",
    "# The following call instantiates DNNClassifier:\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\SIATCO~1.SLA\\AppData\\Local\\Temp\\tmp852m4dl6\\model.ckpt.\n",
      "INFO:tensorflow:loss = 18.084215, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into C:\\Users\\SIATCO~1.SLA\\AppData\\Local\\Temp\\tmp852m4dl6\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.6797957.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x2192c4f6400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating a tf.Estimator.DNNClassifier creates a framework for learning the model.\n",
    "# Basically, we've wired a network but haven't yet let data flow through it.\n",
    "# To train the neural network, call the Estimator object's train method.\n",
    "batch_size = 10\n",
    "train_steps = 100 # You may increase to 1000 to 10000\n",
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(train_feature, train_label, batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-23-09:10:18\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\SIATCO~1.SLA\\AppData\\Local\\Temp\\tmp852m4dl6\\model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-23-09:10:18\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.93333334, average_loss = 0.36960983, global_step = 100, loss = 3.6960983\n",
      "\n",
      "Test set accuracy: 0.933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To evaluate a model's effectiveness, each Estimator provides an evaluate method.\n",
    "# The call to classifier.evaluate is similar to the call to classifier.train.\n",
    "# classifier.evaluate must get its examples from the test set rather than the training set.\n",
    "# In other words, to fairly assess a model's effectiveness, \n",
    "# the examples used to evaluate a model must be different from the examples used to train the model.\n",
    "# The eval_input_fn function serves a batch of examples from the test set\n",
    "eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(test_feature, test_label, batch_size))\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's use the trained model to make some predictions on unlabeled examples;\n",
    "# that is, on examples that contain features but not a label.\n",
    "# For now, we're simply going to manually provide the following four unlabeled examples:\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9, 5.0],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1, 3.0],\n",
    "    'PetalLength': [1.7, 4.2, 5.4, 2.0],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1, 1.0],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Every Estimator provides a predict method, which premade_estimator.py calls as follows:\n",
    "predictions = classifier.predict(\n",
    "        input_fn=lambda:eval_input_fn(predict_x, labels=None, batch_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\SIATCO~1.SLA\\AppData\\Local\\Temp\\tmp852m4dl6\\model.ckpt-100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'class_ids': array([0], dtype=int64),\n",
       "  'classes': array([b'0'], dtype=object),\n",
       "  'logits': array([ 3.0198143, -1.3653761, -0.8638741], dtype=float32),\n",
       "  'probabilities': array([0.96802115, 0.01206204, 0.01991684], dtype=float32)},\n",
       " {'class_ids': array([1], dtype=int64),\n",
       "  'classes': array([b'1'], dtype=object),\n",
       "  'logits': array([-0.60536885,  0.6419004 ,  0.04566121], dtype=float32),\n",
       "  'probabilities': array([0.15629055, 0.54402   , 0.29968944], dtype=float32)},\n",
       " {'class_ids': array([2], dtype=int64),\n",
       "  'classes': array([b'2'], dtype=object),\n",
       "  'logits': array([-0.9931713 ,  0.62524873,  2.102529  ], dtype=float32),\n",
       "  'probabilities': array([0.03552673, 0.17923632, 0.78523695], dtype=float32)},\n",
       " {'class_ids': array([0], dtype=int64),\n",
       "  'classes': array([b'0'], dtype=object),\n",
       "  'logits': array([ 1.8591244, -0.7018992, -0.6718559], dtype=float32),\n",
       "  'probabilities': array([0.86444867, 0.0667576 , 0.06879365], dtype=float32)}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The predict method returns a python iterable, yielding a dictionary of prediction results for each example.\n",
    "# This dictionary contains several keys. The probabilities key holds a list of three floating-point values, \n",
    "# each representing the probability that the input example is a particular Iris species. \n",
    "# For example, consider the following probabilities list\n",
    "\n",
    "list_predictions = list(predictions)\n",
    "list_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'logits': array([ 3.0198143, -1.3653761, -0.8638741], dtype=float32), 'probabilities': array([0.96802115, 0.01206204, 0.01991684], dtype=float32), 'class_ids': array([0], dtype=int64), 'classes': array([b'0'], dtype=object)}\n",
      "\n",
      " {'logits': array([-0.60536885,  0.6419004 ,  0.04566121], dtype=float32), 'probabilities': array([0.15629055, 0.54402   , 0.29968944], dtype=float32), 'class_ids': array([1], dtype=int64), 'classes': array([b'1'], dtype=object)}\n",
      "\n",
      " {'logits': array([-0.9931713 ,  0.62524873,  2.102529  ], dtype=float32), 'probabilities': array([0.03552673, 0.17923632, 0.78523695], dtype=float32), 'class_ids': array([2], dtype=int64), 'classes': array([b'2'], dtype=object)}\n",
      "\n",
      " {'logits': array([ 1.8591244, -0.7018992, -0.6718559], dtype=float32), 'probabilities': array([0.86444867, 0.0667576 , 0.06879365], dtype=float32), 'class_ids': array([0], dtype=int64), 'classes': array([b'0'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "# Same probabilities list with iterate code\n",
    "for pred_dict in list_predictions:\n",
    "    print('\\n', pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"Setosa\" (96.8%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (54.4%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (78.5%), expected \"Virginica\"\n",
      "\n",
      "Prediction is \"Setosa\" (86.4%), expected \"Ask a botanist\"\n"
     ]
    }
   ],
   "source": [
    "expected = [\"Setosa\", \"Versicolor\", \"Virginica\",\"Ask a botanist\"]\n",
    "for pred_dict, expec in zip(list_predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "    print(template.format(SPECIES[class_id], 100 * probability, expec))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This document provides a short introduction to machine learning.\n",
    "# Because premade_estimators.py relies on high-level APIs, much of the mathematical complexity in machine learning is hidden.\n",
    "# If you intend to become more proficient in machine learning, we recommend ultimately learning more about gradient descent,\n",
    "# batching, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enjoy it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
